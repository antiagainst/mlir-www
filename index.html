<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>MLIR: Multi-Level IR Compiler Framework</title><meta name=description content="&lt;insert something insighful here&gt;"><meta name=generator content="Hugo 0.59.1"><link href=https://joker-eph.github.io/www-mlir/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://joker-eph.github.io/www-mlir/><link rel=stylesheet href=https://joker-eph.github.io/www-mlir/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://joker-eph.github.io/www-mlir/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://joker-eph.github.io/www-mlir/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><img src=https://joker-eph.github.io/www-mlir//mlir-logo.png width=40px>MLIR: Multi-Level IR Compiler Framework</h1><a href=https://github.com/llvm/llvm-project/mlir class=github><i class="fab fa-github"></i></a><p class=description>&lt;insert something insighful here&gt;</p></header><div class=global-menu><nav><ul><li><a href=/www-mlir/getting_started/faq/>FAQ</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llvm-project/mlir>Forums</a></li><li class=child><a href=https://discord.gg/JUQUPAZ>Chat</a></li></ul></li></ul></nav></div><div class=content-container><main><h1 id=multi-level-intermediate-representation-overview>Multi-Level Intermediate Representation Overview</h1><p>The MLIR project aims to define a common intermediate representation (IR) that
will unify the infrastructure required to execute high performance machine
learning models in TensorFlow and similar ML frameworks. This project will
include the application of HPC techniques, along with integration of search
algorithms like reinforcement learning. This project aims to reduce the cost to
bring up new hardware, and improve usability for existing TensorFlow users.</p><p>Note that this repository contains the core of the MLIR framework. The
TensorFlow compilers we are building on top of MLIR will be part of the
main TensorFlow repository soon.</p><h1 id=how-to-contribute>How to Contribute</h1><p>Thank you for your interest in contributing to MLIR! If you want to contribute
to MLIR, be sure to review the <a href=CONTRIBUTING.md>contribution guidelines</a>.</p><h2 id=more-resources>More resources</h2><p>For more information on MLIR, please see:</p><ul><li><a href=g3doc/LangRef.md>The MLIR draft specification</a>, which describes the IR
itself.</li><li><a href=g3doc/Rationale.md>The MLIR rationale document</a>, covering motivation
behind some decisions.</li><li>Previous external <a href=#mlir-talks>talks</a>.</li></ul><p>Join the <a href=https://groups.google.com/a/tensorflow.org/forum/#!forum/mlir>MLIR mailing list</a>
to hear about announcements and discussions.</p><p>We also have an <a href=https://github.com/tensorflow/community/blob/master/sigs/mlir/CHARTER.md>MLIR SIG</a>
which was created to enable collaboration and form a strong
engineering-driven open community. We have weekly &lsquo;Open Design Meetings&rsquo;. If you’d like
to discuss a particular topic or have questions, please add it to the <a href=https://docs.google.com/document/d/1y_9f1AbfgcoVdJh4_aM6-BaSHvrHl8zuA5G4jv_94K8/edit#>agenda doc</a>.
Details on how to join the meeting are in the agenda doc. You
should also get an invite when you join the mailing list.</p><p>Please be mindful of the <a href=https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md>TensorFlow Code of Conduct</a>,
which pledges to foster an open and welcoming environment.</p><h2 id=what-is-mlir-for>What is MLIR for?</h2><p>MLIR is intended to be a hybrid IR which can support multiple different
requirements in a unified infrastructure. For example, this includes:</p><ul><li>The ability to represent all TensorFlow graphs, including dynamic shapes,
the user-extensible op ecosystem, TensorFlow variables, etc.</li><li>Optimizations and transformations typically done on a TensorFlow graph, e.g.
in Grappler.</li><li>Quantization and other graph transformations done on a TensorFlow graph or
the TF Lite representation.</li><li>Representation of kernels for ML operations in a form suitable for
optimization.</li><li>Ability to host high-performance-computing-style loop optimizations across
kernels (fusion, loop interchange, tiling, etc) and to transform memory
layouts of data.</li><li>Code generation &ldquo;lowering&rdquo; transformations such as DMA insertion, explicit
cache management, memory tiling, and vectorization for 1D and 2D register
architectures.</li><li>Ability to represent target-specific operations, e.g. the MXU on TPUs.</li></ul><p>MLIR is a common IR that also supports hardware specific operations. Thus,
any investment into the infrastructure surrounding MLIR (e.g. the compiler
passes that work on it) should yield good returns; many targets can use that
infrastructure and will benefit from it.</p><p>MLIR is a powerful representation, but it also has non-goals. We do not try to
support low level machine code generation algorithms (like register allocation
and instruction scheduling). They are a better fit for lower level optimizers
(such as LLVM). Also, we do not intend MLIR to be a source language that
end-users would themselves write kernels in (analogous to CUDA C++). While we
would love to see a kernel language happen someday, that will be an independent
project that compiles down to MLIR.</p><h2 id=compiler-infrastructure>Compiler infrastructure</h2><p>We benefited from experience gained from building other IRs (HLO, LLVM and SIL)
when building MLIR. We will directly adopt existing best practices, e.g. writing
and maintaining an IR spec, building an IR verifier, providing the ability to
dump and parse MLIR files to text, writing extensive unit tests with the
<a href=https://llvm.org/docs/CommandGuide/FileCheck.html>FileCheck</a> tool, and
building the infrastructure as a set of modular libraries that can be combined
in new ways. We plan to use the infrastructure developed by the XLA team for
performance analysis and benchmarking.</p><p>Other lessons have been incorporated and integrated into the design in subtle
ways. For example, LLVM has non-obvious design mistakes that prevent a
multithreaded compiler from working on multiple functions in an LLVM module at
the same time. MLIR solves these problems by having per-function constant pools
and by making references explicit with <code>function_ref</code>.</p><h1 id=getting-started-with-mlir>Getting started with MLIR</h1><p>The following instructions for compiling and testing MLIR assume that you have
<code>git</code>, <a href=https://ninja-build.org/><code>ninja</code></a>, and a working C++ toolchain. In the
future, we aim to align on the same level of platform support as
<a href=https://llvm.org/docs/GettingStarted.html#requirements>LLVM</a>. For now, MLIR
has been tested on Linux and macOS, with recent versions of clang and with
gcc 7.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>git clone https://github.com/llvm/llvm-project.git
git clone https://github.com/tensorflow/mlir llvm-project/llvm/projects/mlir
mkdir llvm-project/build
<span class=nb>cd</span> llvm-project/build
cmake -G Ninja ../llvm -DLLVM_BUILD_EXAMPLES<span class=o>=</span>ON -DLLVM_TARGETS_TO_BUILD<span class=o>=</span><span class=s2>&#34;host&#34;</span>
cmake --build . --target check-mlir</code></pre></div><p>To compile and test on Windows using Visual Studio 2017:</p><div class=highlight><pre class=chroma><code class=language-bat data-lang=bat><span class=c1>REM In shell with Visual Studio environment set up, e.g., with command such as</span>
<span class=c1>REM   $visual-studio-install\Auxiliary\Build\vcvarsall.bat&#34; x64</span>
<span class=c1>REM invoked.</span>
git clone https://github.com/llvm/llvm-project.git
git clone https://github.com/tensorflow/mlir llvm-project\llvm\projects\mlir
<span class=k>mkdir</span> llvm-project\build
<span class=k>cd</span> llvm-project\build
cmake ..\llvm -G <span class=s2>&#34;Visual Studio 15 2017 Win64&#34;</span> -DLLVM_BUILD_EXAMPLES=ON -DLLVM_TARGETS_TO_BUILD=<span class=s2>&#34;host&#34;</span> -DCMAKE_BUILD_TYPE=Release -Thost=x64
cmake --build . --target check-mlir</code></pre></div><p>As a starter, you may try <a href=g3doc/Tutorials/Toy/Ch-1.md>the tutorial</a> on
building a compiler for a Toy language.</p><h1 id=mlir-talks>MLIR talks</h1><ul><li>&ldquo;<a href=https://ai.google/research/pubs/pub48035.pdf>MLIR Primer: A Compiler Infrastructure for the End of Moore’s Law</a>&ldquo;<ul><li>Chris Lattner &amp; Jacques Pienaar, Google at
<a href=https://www.c4ml.org/>Compilers for Machine Learning</a> workshop at
<a href=http://cgo.org/cgo2019/>CGO 2019</a></li></ul></li><li>&ldquo;<a href=https://llvm.org/devmtg/2019-04/talks.html#Keynote_1>MLIR: Multi-Level Intermediate Representation for Compiler
Infrastructure</a>&ldquo;<ul><li>Tatiana Shpeisman &amp; Chris Lattner, Google at
<a href=https://llvm.org/devmtg/2019-04>EuroLLVM 2019</a></li></ul></li><li>&ldquo;<a href=https://llvm.org/devmtg/2019-04/talks.html#Tutorial_1>Tutorial: Building a Compiler with MLIR</a>&ldquo;<ul><li>Mehdi Amini, Jacques Pienaar, Nicolas Vasilache, Google at
<a href=https://llvm.org/devmtg/2019-04>EuroLLVM 2019</a></li></ul></li></ul><nav class=pagination><a class="nav nav-next" href=/www-mlir/getting_started/ title="Getting Started">Next - Getting Started <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><div class=edit-meta>Last updated on 26 Oct 2019<br>Published on 23 Nov 2018<br><a href=https://github.com/joker-eph/www-mlir//edit/master/content/_index.md class=edit-page><i class="fas fa-pen-square"></i>Edit on GitHub</a></div><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=open-menu><ul><li class=active><a href=https://joker-eph.github.io/www-mlir/>Home</a></li><li><a href=/www-mlir/getting_started/>Getting Started</a><ul class=sub-menu><li><a href=/www-mlir/getting_started/faq/>FAQ</a></li><li><a href=/www-mlir/getting_started/developerguide/>Developer Guide</a></li><li><a href=/www-mlir/getting_started/glossary/>Glossary</a></li><li><a href=/www-mlir/getting_started/testingguide/>Testing Guide</a></li></ul></li><li><a href=/www-mlir/docs/>Docs</a><ul class=sub-menu><li><a href=/www-mlir/docs/tutorials/>Tutorials</a><ul class=sub-menu><li><a href=/www-mlir/docs/tutorials/dialectconversion/>Dialect Conversion</a></li><li><a href=/www-mlir/docs/tutorials/interfaces/>Interfaces</a></li><li><a href=/www-mlir/docs/tutorials/traits/>Introduction to Operation Traits</a></li><li><a href=/www-mlir/docs/tutorials/quickstartrewrites/>Quickstart on Graph Rewrite</a></li><li><a href=/www-mlir/docs/tutorials/toy/>Toy Tutorial</a><ul class=sub-menu><li><a href=/www-mlir/docs/tutorials/toy/ch-1/>Chapter 1: Intro</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/www-mlir/docs/tutorials/toy/ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li></ul></li><li><a href=/www-mlir/docs/dialects/>Dialect</a><ul class=sub-menu><li><a href=/www-mlir/docs/dialects/affine/>Affine</a></li><li><a href=/www-mlir/docs/dialects/gpu/>GPU Dialect</a></li><li><a href=/www-mlir/docs/dialects/llvm/>LLVM IR Dialect</a></li><li><a href=/www-mlir/docs/dialects/spir-v/>SPIR-V Dialect</a></li><li><a href=/www-mlir/docs/dialects/standard/>Standard Dialect</a></li><li><a href=/www-mlir/docs/dialects/vector/>Vector Dialect</a></li></ul></li><li><a href=/www-mlir/docs/design/>Design</a><ul class=sub-menu><li><a href=/www-mlir/docs/design/canonicalization/>Canonicalization</a></li><li><a href=/www-mlir/docs/design/rationalesimplifiedpolyhedralform/>Case for a Simplified Polyhedral Form</a></li><li><a href=/www-mlir/docs/design/rationale/>Design Rationale</a></li><li><a href=/www-mlir/docs/design/diagnostics/>Diagnostics Infrastructure</a></li><li><a href=/www-mlir/docs/design/edsc/>EDSC: Declarative Builders</a></li><li><a href=/www-mlir/docs/design/genericdagrewriter/>Generic DAG Rewriter Infrastructure</a></li><li><a href=/www-mlir/docs/design/mlirforgraphalgorithms/>Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/www-mlir/docs/design/quantization/>Quantization</a></li><li><a href=/www-mlir/docs/design/declarativerewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/www-mlir/docs/design/opdefinitions/>Table-driven Operation Definition Specification (ODS)</a></li><li><a href=/www-mlir/docs/design/usageofconst/>Usage of &#39;Const&#39; in MLIR, for core IR types</a></li></ul></li><li><a href=/www-mlir/docs/conversiontollvmdialect/>Conversion to the LLVM Dialect</a></li><li><a href=/www-mlir/docs/langref/>Core Specification</a></li><li><a href=/www-mlir/docs/passes/>Passes</a></li><li><a href=/www-mlir/docs/definingattributesandtypes/>Quickstart tutorial to defining custom dialect attributes and types</a></li><li><a href=/www-mlir/docs/writingapass/>Writing a Pass</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>